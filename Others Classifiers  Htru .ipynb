{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt \n",
    "from pprint import pprint\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import names\n",
    "df = pd.read_csv(\"htru.csv\", names = names.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Profile_mean</th>\n",
       "      <th>Profile_stdev</th>\n",
       "      <th>Profile_skewness</th>\n",
       "      <th>Profile_kurtosis</th>\n",
       "      <th>DM_mean</th>\n",
       "      <th>DM_stdev</th>\n",
       "      <th>DM_skewness</th>\n",
       "      <th>DM_kurtosis</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Profile_mean  Profile_stdev  Profile_skewness  Profile_kurtosis   DM_mean  \\\n",
       "0    140.562500      55.683782         -0.234571         -0.699648  3.199833   \n",
       "1    102.507812      58.882430          0.465318         -0.515088  1.677258   \n",
       "2    103.015625      39.341649          0.323328          1.051164  3.121237   \n",
       "3    136.750000      57.178449         -0.068415         -0.636238  3.642977   \n",
       "4     88.726562      40.672225          0.600866          1.123492  1.178930   \n",
       "\n",
       "    DM_stdev  DM_skewness  DM_kurtosis  class  \n",
       "0  19.110426     7.975532    74.242225      0  \n",
       "1  14.860146    10.576487   127.393580      0  \n",
       "2  21.744669     7.735822    63.171909      0  \n",
       "3  20.959280     6.896499    53.593661      0  \n",
       "4  11.468720    14.269573   252.567306      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to values\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GC Forest\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "sys.path.insert(0, \"lib\")\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json\n",
    "config = load_json(\"./examples/htru.json\")  # layer = 1   k=10   Extree + DTree\n",
    "gc = GCForest(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test \n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = df[:,8]\n",
    "X = df[:,0:8]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of class\n",
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-21 14:31:33,474][cascade_classifier.fit_transform] X_groups_train.shape=[(12528, 8)],y_train.shape=(12528,),X_groups_test.shape=[(5370, 8)],y_test.shape=(5370,)\n",
      "[ 2018-04-21 14:31:33,478][cascade_classifier.fit_transform] group_dims=[8]\n",
      "[ 2018-04-21 14:31:33,480][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-04-21 14:31:33,482][cascade_classifier.fit_transform] group_ends=[8]\n",
      "[ 2018-04-21 14:31:33,484][cascade_classifier.fit_transform] X_train.shape=(12528, 8),X_test.shape=(5370, 8)\n",
      "[ 2018-04-21 14:31:33,487][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(12528, 8), X_cur_test.shape=(5370, 8)\n",
      "[ 2018-04-21 14:31:34,287][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_0.predict)=98.25%\n",
      "[ 2018-04-21 14:31:35,229][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_1.predict)=97.69%\n",
      "[ 2018-04-21 14:31:36,167][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_2.predict)=98.01%\n",
      "[ 2018-04-21 14:31:37,174][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_3.predict)=97.61%\n",
      "[ 2018-04-21 14:31:38,081][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_4.predict)=97.92%\n",
      "[ 2018-04-21 14:31:38,993][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_5.predict)=98.08%\n",
      "[ 2018-04-21 14:31:39,878][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_6.predict)=97.60%\n",
      "[ 2018-04-21 14:31:40,884][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_7.predict)=98.08%\n",
      "[ 2018-04-21 14:31:41,833][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_8.predict)=97.92%\n",
      "[ 2018-04-21 14:31:42,872][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_9.predict)=98.08%\n",
      "[ 2018-04-21 14:31:43,036][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_cv.predict)=97.92%\n",
      "[ 2018-04-21 14:31:43,038][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.test.predict)=98.04%\n",
      "[ 2018-04-21 14:31:43,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_0.predict)=97.93%\n",
      "[ 2018-04-21 14:31:44,521][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_1.predict)=97.85%\n",
      "[ 2018-04-21 14:31:45,346][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_2.predict)=97.61%\n",
      "[ 2018-04-21 14:31:46,159][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_3.predict)=98.16%\n",
      "[ 2018-04-21 14:31:46,929][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_4.predict)=97.77%\n",
      "[ 2018-04-21 14:31:47,724][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_5.predict)=97.76%\n",
      "[ 2018-04-21 14:31:48,511][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_6.predict)=97.92%\n",
      "[ 2018-04-21 14:31:49,298][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_7.predict)=98.00%\n",
      "[ 2018-04-21 14:31:50,141][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_8.predict)=98.24%\n",
      "[ 2018-04-21 14:31:50,943][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_9.predict)=97.92%\n",
      "[ 2018-04-21 14:31:51,103][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_cv.predict)=97.92%\n",
      "[ 2018-04-21 14:31:51,105][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.test.predict)=98.08%\n",
      "[ 2018-04-21 14:31:51,451][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_0.predict)=98.09%\n",
      "[ 2018-04-21 14:31:51,601][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_1.predict)=97.93%\n",
      "[ 2018-04-21 14:31:51,694][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_2.predict)=97.69%\n",
      "[ 2018-04-21 14:31:51,760][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_3.predict)=97.53%\n",
      "[ 2018-04-21 14:31:51,829][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_4.predict)=97.53%\n",
      "[ 2018-04-21 14:31:51,901][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_5.predict)=98.48%\n",
      "[ 2018-04-21 14:31:51,967][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_6.predict)=97.92%\n",
      "[ 2018-04-21 14:31:52,036][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_7.predict)=98.08%\n",
      "[ 2018-04-21 14:31:52,125][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_8.predict)=98.24%\n",
      "[ 2018-04-21 14:31:52,198][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_9.predict)=96.96%\n",
      "[ 2018-04-21 14:31:52,201][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_cv.predict)=97.84%\n",
      "[ 2018-04-21 14:31:52,202][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.test.predict)=97.97%\n",
      "[ 2018-04-21 14:31:52,205][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=97.89%\n",
      "[ 2018-04-21 14:31:52,207][cascade_classifier.calc_accuracy] Accuracy(layer_0 - test.classifier_average)=98.10%\n",
      "[ 2018-04-21 14:31:52,210][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(12528, 14), X_cur_test.shape=(5370, 14)\n",
      "[ 2018-04-21 14:31:53,130][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_0.predict)=98.09%\n",
      "[ 2018-04-21 14:31:54,057][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_1.predict)=97.93%\n",
      "[ 2018-04-21 14:31:55,003][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_2.predict)=97.45%\n",
      "[ 2018-04-21 14:31:56,019][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_3.predict)=97.77%\n",
      "[ 2018-04-21 14:31:56,978][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_4.predict)=98.24%\n",
      "[ 2018-04-21 14:31:57,922][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_5.predict)=97.92%\n",
      "[ 2018-04-21 14:31:58,817][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_6.predict)=98.32%\n",
      "[ 2018-04-21 14:31:59,710][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_7.predict)=97.92%\n",
      "[ 2018-04-21 14:32:00,660][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_8.predict)=98.00%\n",
      "[ 2018-04-21 14:32:01,580][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_9.predict)=98.00%\n",
      "[ 2018-04-21 14:32:01,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_cv.predict)=97.96%\n",
      "[ 2018-04-21 14:32:01,738][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.test.predict)=98.04%\n",
      "[ 2018-04-21 14:32:02,373][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_0.predict)=98.01%\n",
      "[ 2018-04-21 14:32:03,309][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_1.predict)=98.48%\n",
      "[ 2018-04-21 14:32:04,261][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_2.predict)=97.61%\n",
      "[ 2018-04-21 14:32:05,082][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_3.predict)=97.69%\n",
      "[ 2018-04-21 14:32:05,888][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_4.predict)=97.45%\n",
      "[ 2018-04-21 14:32:06,641][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_5.predict)=98.00%\n",
      "[ 2018-04-21 14:32:07,401][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_6.predict)=98.00%\n",
      "[ 2018-04-21 14:32:08,362][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_7.predict)=98.72%\n",
      "[ 2018-04-21 14:32:09,129][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_8.predict)=97.52%\n",
      "[ 2018-04-21 14:32:09,958][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_9.predict)=97.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-21 14:32:10,123][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_cv.predict)=97.93%\n",
      "[ 2018-04-21 14:32:10,126][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.test.predict)=98.06%\n",
      "[ 2018-04-21 14:32:10,299][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_0.predict)=98.09%\n",
      "[ 2018-04-21 14:32:10,488][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_1.predict)=97.61%\n",
      "[ 2018-04-21 14:32:10,703][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_2.predict)=98.41%\n",
      "[ 2018-04-21 14:32:10,827][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_3.predict)=97.61%\n",
      "[ 2018-04-21 14:32:10,958][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_4.predict)=97.92%\n",
      "[ 2018-04-21 14:32:11,075][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_5.predict)=97.92%\n",
      "[ 2018-04-21 14:32:11,196][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_6.predict)=97.84%\n",
      "[ 2018-04-21 14:32:11,324][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_7.predict)=97.92%\n",
      "[ 2018-04-21 14:32:11,479][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_8.predict)=98.24%\n",
      "[ 2018-04-21 14:32:11,634][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_9.predict)=97.84%\n",
      "[ 2018-04-21 14:32:11,637][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_cv.predict)=97.94%\n",
      "[ 2018-04-21 14:32:11,638][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.test.predict)=98.06%\n",
      "[ 2018-04-21 14:32:11,640][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=98.00%\n",
      "[ 2018-04-21 14:32:11,642][cascade_classifier.calc_accuracy] Accuracy(layer_1 - test.classifier_average)=98.04%\n",
      "[ 2018-04-21 14:32:11,644][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(12528, 14), X_cur_test.shape=(5370, 14)\n",
      "[ 2018-04-21 14:32:12,416][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_0.predict)=98.25%\n",
      "[ 2018-04-21 14:32:13,244][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_1.predict)=98.01%\n",
      "[ 2018-04-21 14:32:14,136][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_2.predict)=97.21%\n",
      "[ 2018-04-21 14:32:15,125][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_3.predict)=98.64%\n",
      "[ 2018-04-21 14:32:16,051][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_4.predict)=97.85%\n",
      "[ 2018-04-21 14:32:17,013][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_5.predict)=97.60%\n",
      "[ 2018-04-21 14:32:17,854][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_6.predict)=98.24%\n",
      "[ 2018-04-21 14:32:18,575][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_7.predict)=98.48%\n",
      "[ 2018-04-21 14:32:19,672][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_8.predict)=97.60%\n",
      "[ 2018-04-21 14:32:20,552][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_9.predict)=97.12%\n",
      "[ 2018-04-21 14:32:20,765][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_cv.predict)=97.90%\n",
      "[ 2018-04-21 14:32:20,767][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.test.predict)=98.03%\n",
      "[ 2018-04-21 14:32:21,501][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_0.predict)=97.61%\n",
      "[ 2018-04-21 14:32:22,329][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_1.predict)=98.25%\n",
      "[ 2018-04-21 14:32:23,084][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_2.predict)=98.01%\n",
      "[ 2018-04-21 14:32:24,026][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_3.predict)=97.13%\n",
      "[ 2018-04-21 14:32:25,051][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_4.predict)=97.29%\n",
      "[ 2018-04-21 14:32:26,155][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_5.predict)=97.76%\n",
      "[ 2018-04-21 14:32:27,083][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_6.predict)=96.88%\n",
      "[ 2018-04-21 14:32:27,974][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_7.predict)=98.24%\n",
      "[ 2018-04-21 14:32:28,711][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_8.predict)=98.72%\n",
      "[ 2018-04-21 14:32:29,401][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_9.predict)=98.72%\n",
      "[ 2018-04-21 14:32:29,546][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_cv.predict)=97.86%\n",
      "[ 2018-04-21 14:32:29,547][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.test.predict)=98.12%\n",
      "[ 2018-04-21 14:32:29,753][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_0.predict)=97.77%\n",
      "[ 2018-04-21 14:32:30,038][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_1.predict)=97.77%\n",
      "[ 2018-04-21 14:32:30,309][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_2.predict)=98.25%\n",
      "[ 2018-04-21 14:32:30,499][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_3.predict)=97.77%\n",
      "[ 2018-04-21 14:32:30,675][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_4.predict)=97.69%\n",
      "[ 2018-04-21 14:32:30,835][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_5.predict)=98.24%\n",
      "[ 2018-04-21 14:32:30,945][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_6.predict)=97.36%\n",
      "[ 2018-04-21 14:32:31,077][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_7.predict)=98.32%\n",
      "[ 2018-04-21 14:32:31,205][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_8.predict)=97.68%\n",
      "[ 2018-04-21 14:32:31,349][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_9.predict)=98.48%\n",
      "[ 2018-04-21 14:32:31,350][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_cv.predict)=97.93%\n",
      "[ 2018-04-21 14:32:31,352][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.test.predict)=98.03%\n",
      "[ 2018-04-21 14:32:31,353][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=97.92%\n",
      "[ 2018-04-21 14:32:31,355][cascade_classifier.calc_accuracy] Accuracy(layer_2 - test.classifier_average)=98.10%\n",
      "[ 2018-04-21 14:32:31,357][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(12528, 14), X_cur_test.shape=(5370, 14)\n",
      "[ 2018-04-21 14:32:32,040][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_0.predict)=97.45%\n",
      "[ 2018-04-21 14:32:33,132][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_1.predict)=98.01%\n",
      "[ 2018-04-21 14:32:33,900][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_2.predict)=98.17%\n",
      "[ 2018-04-21 14:32:34,703][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_3.predict)=98.72%\n",
      "[ 2018-04-21 14:32:35,642][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_4.predict)=97.92%\n",
      "[ 2018-04-21 14:32:36,357][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_5.predict)=97.60%\n",
      "[ 2018-04-21 14:32:37,108][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_6.predict)=98.24%\n",
      "[ 2018-04-21 14:32:38,231][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_7.predict)=97.60%\n",
      "[ 2018-04-21 14:32:38,967][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_8.predict)=98.16%\n",
      "[ 2018-04-21 14:32:39,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_9.predict)=97.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-21 14:32:39,990][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_cv.predict)=97.98%\n",
      "[ 2018-04-21 14:32:39,993][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.test.predict)=98.18%\n",
      "[ 2018-04-21 14:32:40,708][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_0.predict)=97.77%\n",
      "[ 2018-04-21 14:32:41,444][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_1.predict)=98.01%\n",
      "[ 2018-04-21 14:32:42,172][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_2.predict)=98.64%\n",
      "[ 2018-04-21 14:32:43,170][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_3.predict)=97.69%\n",
      "[ 2018-04-21 14:32:43,826][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_4.predict)=97.92%\n",
      "[ 2018-04-21 14:32:44,644][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_5.predict)=98.24%\n",
      "[ 2018-04-21 14:32:45,703][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_6.predict)=97.52%\n",
      "[ 2018-04-21 14:32:46,415][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_7.predict)=97.92%\n",
      "[ 2018-04-21 14:32:47,183][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_8.predict)=97.60%\n",
      "[ 2018-04-21 14:32:48,356][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_9.predict)=97.44%\n",
      "[ 2018-04-21 14:32:48,489][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_cv.predict)=97.88%\n",
      "[ 2018-04-21 14:32:48,490][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.test.predict)=98.10%\n",
      "[ 2018-04-21 14:32:48,648][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_0.predict)=98.09%\n",
      "[ 2018-04-21 14:32:48,783][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_1.predict)=97.53%\n",
      "[ 2018-04-21 14:32:48,945][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_2.predict)=98.41%\n",
      "[ 2018-04-21 14:32:49,100][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_3.predict)=98.16%\n",
      "[ 2018-04-21 14:32:49,226][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_4.predict)=97.92%\n",
      "[ 2018-04-21 14:32:49,364][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_5.predict)=97.92%\n",
      "[ 2018-04-21 14:32:49,553][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_6.predict)=97.92%\n",
      "[ 2018-04-21 14:32:49,722][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_7.predict)=97.20%\n",
      "[ 2018-04-21 14:32:50,039][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_8.predict)=97.36%\n",
      "[ 2018-04-21 14:32:50,262][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_9.predict)=98.24%\n",
      "[ 2018-04-21 14:32:50,285][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_cv.predict)=97.88%\n",
      "[ 2018-04-21 14:32:50,287][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.test.predict)=98.04%\n",
      "[ 2018-04-21 14:32:50,290][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=97.92%\n",
      "[ 2018-04-21 14:32:50,292][cascade_classifier.calc_accuracy] Accuracy(layer_3 - test.classifier_average)=98.12%\n",
      "[ 2018-04-21 14:32:50,296][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(12528, 14), X_cur_test.shape=(5370, 14)\n",
      "[ 2018-04-21 14:32:51,136][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_0.predict)=97.45%\n",
      "[ 2018-04-21 14:32:51,981][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_1.predict)=98.01%\n",
      "[ 2018-04-21 14:32:53,184][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_2.predict)=98.80%\n",
      "[ 2018-04-21 14:32:54,068][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_3.predict)=97.77%\n",
      "[ 2018-04-21 14:32:55,053][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_4.predict)=98.32%\n",
      "[ 2018-04-21 14:32:56,156][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_5.predict)=98.08%\n",
      "[ 2018-04-21 14:32:56,970][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_6.predict)=97.12%\n",
      "[ 2018-04-21 14:32:58,223][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_7.predict)=98.00%\n",
      "[ 2018-04-21 14:32:58,917][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_8.predict)=98.48%\n",
      "[ 2018-04-21 14:32:59,757][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_9.predict)=98.00%\n",
      "[ 2018-04-21 14:32:59,988][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_cv.predict)=98.00%\n",
      "[ 2018-04-21 14:32:59,993][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.test.predict)=98.14%\n",
      "[ 2018-04-21 14:33:00,902][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_0.predict)=98.01%\n",
      "[ 2018-04-21 14:33:01,642][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_1.predict)=98.56%\n",
      "[ 2018-04-21 14:33:02,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_2.predict)=97.61%\n",
      "[ 2018-04-21 14:33:03,471][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_3.predict)=97.21%\n",
      "[ 2018-04-21 14:33:04,191][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_4.predict)=98.24%\n",
      "[ 2018-04-21 14:33:05,305][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_5.predict)=98.24%\n",
      "[ 2018-04-21 14:33:06,272][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_6.predict)=97.36%\n",
      "[ 2018-04-21 14:33:06,949][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_7.predict)=98.48%\n",
      "[ 2018-04-21 14:33:07,977][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_8.predict)=97.68%\n",
      "[ 2018-04-21 14:33:08,701][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_9.predict)=98.08%\n",
      "[ 2018-04-21 14:33:08,827][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_cv.predict)=97.95%\n",
      "[ 2018-04-21 14:33:08,828][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.test.predict)=98.03%\n",
      "[ 2018-04-21 14:33:08,987][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_0.predict)=98.64%\n",
      "[ 2018-04-21 14:33:09,120][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_1.predict)=97.93%\n",
      "[ 2018-04-21 14:33:09,250][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_2.predict)=97.93%\n",
      "[ 2018-04-21 14:33:09,378][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_3.predict)=97.53%\n",
      "[ 2018-04-21 14:33:09,501][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_4.predict)=97.77%\n",
      "[ 2018-04-21 14:33:09,662][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_5.predict)=98.64%\n",
      "[ 2018-04-21 14:33:09,862][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_6.predict)=98.32%\n",
      "[ 2018-04-21 14:33:10,091][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_7.predict)=97.76%\n",
      "[ 2018-04-21 14:33:10,312][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_8.predict)=97.68%\n",
      "[ 2018-04-21 14:33:10,549][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_9.predict)=97.84%\n",
      "[ 2018-04-21 14:33:10,562][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_cv.predict)=98.00%\n",
      "[ 2018-04-21 14:33:10,565][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.test.predict)=98.12%\n",
      "[ 2018-04-21 14:33:10,569][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=97.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-21 14:33:10,573][cascade_classifier.calc_accuracy] Accuracy(layer_4 - test.classifier_average)=98.12%\n",
      "[ 2018-04-21 14:33:10,575][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=2, accuracy_train=98.00%, accuracy_test=98.04%\n"
     ]
    }
   ],
   "source": [
    "    # X_enc is the concatenated predict_proba result of each estimators of the last layer of the GCForest model\n",
    "    # X_enc.shape =\n",
    "    #   (n_datas, n_estimators * n_classes): If cascade is provided\n",
    "    #   (n_datas, n_estimators * n_classes, dimX, dimY): If only finegrained part is provided\n",
    "    # You can also pass X_test, y_test to fit_transform method, then the accracy on test data will be logged when training.\n",
    "X_train_enc, X_test_enc = gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test)\n",
    "    # WARNING: if you set gc.set_keep_model_in_mem(True), you would have to use\n",
    "    # gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test) to evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-21 00:17:51,084][cascade_classifier.transform] X_groups_test.shape=[(5370, 8)]\n",
      "[ 2018-04-21 00:17:51,089][cascade_classifier.transform] group_dims=[8]\n",
      "[ 2018-04-21 00:17:51,090][cascade_classifier.transform] X_test.shape=(5370, 8)\n",
      "[ 2018-04-21 00:17:51,092][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5370, 8)\n",
      "[ 2018-04-21 00:17:53,652][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5370, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GCForest = 98.044693 %\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = gc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of GCForest = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 2s 152us/step - loss: 0.2275 - acc: 0.9369\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0949 - acc: 0.9728\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0905 - acc: 0.9739: 0s - loss: 0.1055 -\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0914 - acc: 0.9743\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0885 - acc: 0.9744\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0883 - acc: 0.9754\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0881 - acc: 0.9740\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0872 - acc: 0.9751\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0877 - acc: 0.9744\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0866 - acc: 0.9745\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0856 - acc: 0.9754\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0853 - acc: 0.9746\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0844 - acc: 0.9753\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0846 - acc: 0.9754\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0836 - acc: 0.9752\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0840 - acc: 0.9754\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0844 - acc: 0.9749\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0833 - acc: 0.9750\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0832 - acc: 0.9743\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0832 - acc: 0.9758\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0825 - acc: 0.9761\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0822 - acc: 0.9753: 0s - loss: 0.0878 -\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0826 - acc: 0.9752\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0816 - acc: 0.9758\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0824 - acc: 0.9753\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0822 - acc: 0.9761\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0816 - acc: 0.9759\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0818 - acc: 0.9750\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 77us/step - loss: 0.0814 - acc: 0.9758\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0829 - acc: 0.9760\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0817 - acc: 0.9753\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0816 - acc: 0.9759\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0808 - acc: 0.9759\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0806 - acc: 0.9759\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0812 - acc: 0.9760\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0818 - acc: 0.9763\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0811 - acc: 0.9756\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0800 - acc: 0.9761\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0802 - acc: 0.9768\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0803 - acc: 0.9766\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0803 - acc: 0.9755\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0797 - acc: 0.9766\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0800 - acc: 0.9766\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 50us/step - loss: 0.0800 - acc: 0.9768\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 50us/step - loss: 0.0791 - acc: 0.9769\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 49us/step - loss: 0.0798 - acc: 0.9761\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0797 - acc: 0.9763\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0792 - acc: 0.9765\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0794 - acc: 0.9762\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0789 - acc: 0.9763\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0787 - acc: 0.9769\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0786 - acc: 0.9772\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0780 - acc: 0.9774\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0782 - acc: 0.9771\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0790 - acc: 0.9762\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0776 - acc: 0.9773\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0776 - acc: 0.9770\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0771 - acc: 0.9772\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0770 - acc: 0.9764\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0772 - acc: 0.9778\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0773 - acc: 0.9776\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0774 - acc: 0.9769\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0763 - acc: 0.9776\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0767 - acc: 0.9778\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0768 - acc: 0.9773\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0758 - acc: 0.9776\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0768 - acc: 0.9769\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0763 - acc: 0.9769\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - ETA: 0s - loss: 0.0766 - acc: 0.977 - 1s 63us/step - loss: 0.0752 - acc: 0.9775\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 82us/step - loss: 0.0753 - acc: 0.9778\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0751 - acc: 0.9775\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0755 - acc: 0.9780\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0755 - acc: 0.9776\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0757 - acc: 0.9775\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0747 - acc: 0.9771\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0748 - acc: 0.9775\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0750 - acc: 0.9768\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0746 - acc: 0.9782\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0748 - acc: 0.9784\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0740 - acc: 0.9782\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0750 - acc: 0.9774\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0736 - acc: 0.9774\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0736 - acc: 0.9783\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0738 - acc: 0.9787\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0735 - acc: 0.9791\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0744 - acc: 0.9781\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 50us/step - loss: 0.0740 - acc: 0.9783\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0748 - acc: 0.9774\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0738 - acc: 0.9784\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0743 - acc: 0.9784\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0727 - acc: 0.9793\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0745 - acc: 0.9784\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0726 - acc: 0.9778\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0733 - acc: 0.9782\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0725 - acc: 0.9781\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0723 - acc: 0.9785\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0735 - acc: 0.9782\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 50us/step - loss: 0.0720 - acc: 0.9784\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0729 - acc: 0.9782\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0731 - acc: 0.9789\n",
      "1253/1253 [==============================] - 0s 58us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 79us/step - loss: 0.2311 - acc: 0.9285\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.1021 - acc: 0.9722\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0921 - acc: 0.9741\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 49us/step - loss: 0.0916 - acc: 0.9743\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0933 - acc: 0.9741\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0906 - acc: 0.9744\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0902 - acc: 0.9738\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0892 - acc: 0.9747\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0906 - acc: 0.9737\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0906 - acc: 0.9744\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0887 - acc: 0.9749\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0890 - acc: 0.9750\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0885 - acc: 0.9746\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0880 - acc: 0.9752\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0866 - acc: 0.9749\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0873 - acc: 0.9746\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0872 - acc: 0.9743\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0862 - acc: 0.9746\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0863 - acc: 0.9753\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0858 - acc: 0.9754\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0848 - acc: 0.9753\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0851 - acc: 0.9756\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0848 - acc: 0.9749\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0845 - acc: 0.9752\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0838 - acc: 0.9759\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0839 - acc: 0.9753\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0834 - acc: 0.9755\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0852 - acc: 0.9747\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0836 - acc: 0.9753\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0827 - acc: 0.9761\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0831 - acc: 0.9761\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0825 - acc: 0.9759\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0831 - acc: 0.9753\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0827 - acc: 0.9756\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0833 - acc: 0.9758\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0830 - acc: 0.9756\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0821 - acc: 0.9753\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0827 - acc: 0.9752\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0815 - acc: 0.9766: 0s - loss: 0.0941 -\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0818 - acc: 0.9755\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0817 - acc: 0.9758\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0813 - acc: 0.9764\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0821 - acc: 0.9758\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0825 - acc: 0.9758\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0820 - acc: 0.9755\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0805 - acc: 0.9761\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0805 - acc: 0.9758\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0803 - acc: 0.9757\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0811 - acc: 0.9767\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0810 - acc: 0.9755\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0804 - acc: 0.9765\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0803 - acc: 0.9764\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0796 - acc: 0.9763\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0798 - acc: 0.9761\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0802 - acc: 0.9762\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0799 - acc: 0.9761\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0797 - acc: 0.9768\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0786 - acc: 0.9775\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0788 - acc: 0.9770\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0796 - acc: 0.9764\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0787 - acc: 0.9774\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0784 - acc: 0.9768\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0780 - acc: 0.9770\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0787 - acc: 0.9774\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0782 - acc: 0.9774\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0775 - acc: 0.9769\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0779 - acc: 0.9768\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0767 - acc: 0.9770\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0772 - acc: 0.9781\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0772 - acc: 0.9770\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0772 - acc: 0.9778\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0770 - acc: 0.9775\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0766 - acc: 0.9775\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0759 - acc: 0.9769\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0755 - acc: 0.9777\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0759 - acc: 0.9778\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0754 - acc: 0.9781\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 51us/step - loss: 0.0763 - acc: 0.9778\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0749 - acc: 0.9781\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0746 - acc: 0.9784\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0748 - acc: 0.9778\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0760 - acc: 0.9783\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0759 - acc: 0.9777\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0739 - acc: 0.9776\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0758 - acc: 0.9774\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0744 - acc: 0.9790\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0751 - acc: 0.9785\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0746 - acc: 0.9779\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0747 - acc: 0.9788\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0742 - acc: 0.9780\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0750 - acc: 0.9778\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0741 - acc: 0.9785\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0730 - acc: 0.9791\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0747 - acc: 0.9782\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0739 - acc: 0.9776\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0745 - acc: 0.9776\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0750 - acc: 0.9784\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0746 - acc: 0.9773: 0s - loss: 0.0805 \n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0736 - acc: 0.9788\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0747 - acc: 0.9784\n",
      "1253/1253 [==============================] - 0s 72us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 82us/step - loss: 0.2411 - acc: 0.9055\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.1388 - acc: 0.9530\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.1275 - acc: 0.9721\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.1219 - acc: 0.9730\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.1106 - acc: 0.9734\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0913 - acc: 0.9733\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0912 - acc: 0.9737\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0896 - acc: 0.9742\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0893 - acc: 0.9737\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0899 - acc: 0.9738\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0889 - acc: 0.9747\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0877 - acc: 0.9743\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0873 - acc: 0.9737\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 79us/step - loss: 0.0880 - acc: 0.9737\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0879 - acc: 0.9735\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0875 - acc: 0.9745\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0876 - acc: 0.9738\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0879 - acc: 0.9742\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0872 - acc: 0.9744\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0869 - acc: 0.9741\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0871 - acc: 0.9741\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0868 - acc: 0.9742\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0865 - acc: 0.9743\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0873 - acc: 0.9743\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0863 - acc: 0.9742\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0858 - acc: 0.9745\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0862 - acc: 0.9749\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0863 - acc: 0.9750\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0854 - acc: 0.9743\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0858 - acc: 0.9739\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0856 - acc: 0.9753\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0845 - acc: 0.9749\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0847 - acc: 0.9748\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0842 - acc: 0.9755\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0845 - acc: 0.9749\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0839 - acc: 0.9745\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0839 - acc: 0.9751\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0831 - acc: 0.9751\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0832 - acc: 0.9750\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0831 - acc: 0.9751\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0835 - acc: 0.9749\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0830 - acc: 0.9750\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0832 - acc: 0.9750\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0829 - acc: 0.9749\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0830 - acc: 0.9756\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0828 - acc: 0.9756\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0823 - acc: 0.9750\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0828 - acc: 0.9754\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0822 - acc: 0.9759\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0817 - acc: 0.9756\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0819 - acc: 0.9756\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0823 - acc: 0.9749\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0816 - acc: 0.9756\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0825 - acc: 0.9753\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0816 - acc: 0.9757\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0813 - acc: 0.9754\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0816 - acc: 0.9753\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0814 - acc: 0.9757\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0813 - acc: 0.9764\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0813 - acc: 0.9755\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0809 - acc: 0.9763\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0809 - acc: 0.9753\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0808 - acc: 0.9754\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0813 - acc: 0.9753\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0804 - acc: 0.9758\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0805 - acc: 0.9756\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0805 - acc: 0.9757\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0800 - acc: 0.9761\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0809 - acc: 0.9758\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0804 - acc: 0.9754\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0804 - acc: 0.9759\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0800 - acc: 0.9753\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0799 - acc: 0.9756\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0799 - acc: 0.9758\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0802 - acc: 0.9760\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0794 - acc: 0.9759\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0803 - acc: 0.9761\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0790 - acc: 0.9763\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0795 - acc: 0.9768\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0790 - acc: 0.9756: 0s - loss: 0.0680\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0789 - acc: 0.9759\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0786 - acc: 0.9767\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0802 - acc: 0.9757\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0793 - acc: 0.9769\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0787 - acc: 0.9765\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0800 - acc: 0.9755\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0789 - acc: 0.9765\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0783 - acc: 0.9761\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0772 - acc: 0.9771\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0790 - acc: 0.9761\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0788 - acc: 0.9761\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0791 - acc: 0.9767\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0781 - acc: 0.9762\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0775 - acc: 0.9764\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0775 - acc: 0.9764\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0768 - acc: 0.9768\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0784 - acc: 0.9760\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0775 - acc: 0.9765\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0764 - acc: 0.9761\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 79us/step - loss: 0.0768 - acc: 0.9770\n",
      "1253/1253 [==============================] - 0s 114us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 85us/step - loss: 0.2294 - acc: 0.9315\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0925 - acc: 0.9728\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0909 - acc: 0.9737\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0890 - acc: 0.9745\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0876 - acc: 0.9754\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0884 - acc: 0.9753\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0877 - acc: 0.9743\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0867 - acc: 0.9744\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0869 - acc: 0.9749\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0870 - acc: 0.9742\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0858 - acc: 0.9755\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0857 - acc: 0.9745\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0867 - acc: 0.9753\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0864 - acc: 0.9755\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0857 - acc: 0.9743\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0846 - acc: 0.9754\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0849 - acc: 0.9752\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0844 - acc: 0.9753\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0850 - acc: 0.9740\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0847 - acc: 0.9754\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0839 - acc: 0.9754\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0840 - acc: 0.9754\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0831 - acc: 0.9750: 0s - loss: 0.0895\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0834 - acc: 0.9757\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0847 - acc: 0.9753\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0835 - acc: 0.9763\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0831 - acc: 0.9752\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0834 - acc: 0.9751\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0834 - acc: 0.9757\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0827 - acc: 0.9761\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0831 - acc: 0.9759\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0825 - acc: 0.9760\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0829 - acc: 0.9753\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0827 - acc: 0.9751\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0828 - acc: 0.9760\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0841 - acc: 0.9758\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0817 - acc: 0.9769\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0818 - acc: 0.9764\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0818 - acc: 0.9761\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0821 - acc: 0.9761\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0822 - acc: 0.9762\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0814 - acc: 0.9768\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0821 - acc: 0.9762\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0815 - acc: 0.9765\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0810 - acc: 0.9766\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0813 - acc: 0.9769\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0810 - acc: 0.9768\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0810 - acc: 0.9768\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0815 - acc: 0.9764\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0816 - acc: 0.9765\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0810 - acc: 0.9757\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0807 - acc: 0.9764\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0806 - acc: 0.9769\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0802 - acc: 0.9761\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0805 - acc: 0.9765\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0802 - acc: 0.9769\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0803 - acc: 0.9770\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0799 - acc: 0.9761\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0799 - acc: 0.9774\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0797 - acc: 0.9783\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0805 - acc: 0.9775\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0789 - acc: 0.9770\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0804 - acc: 0.9768\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0796 - acc: 0.9772\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0793 - acc: 0.9774\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0791 - acc: 0.9764\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0787 - acc: 0.9773\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0796 - acc: 0.9762\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0782 - acc: 0.9779\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0783 - acc: 0.9775\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0778 - acc: 0.9779\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0779 - acc: 0.9782\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0786 - acc: 0.9764\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0788 - acc: 0.9768\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0789 - acc: 0.9774\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0772 - acc: 0.9776\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0773 - acc: 0.9778\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0772 - acc: 0.9772\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0788 - acc: 0.9775\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0768 - acc: 0.9780\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0768 - acc: 0.9775\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0760 - acc: 0.9780\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0766 - acc: 0.9774\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 82us/step - loss: 0.0762 - acc: 0.9784\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0768 - acc: 0.9775\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0755 - acc: 0.9784\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0752 - acc: 0.9785\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0759 - acc: 0.9781\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0760 - acc: 0.9778\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0750 - acc: 0.9787\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0754 - acc: 0.9785\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0764 - acc: 0.9777\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0750 - acc: 0.9783\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0747 - acc: 0.9786\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0745 - acc: 0.9786\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0740 - acc: 0.9792\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0754 - acc: 0.9784\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0740 - acc: 0.9784\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0744 - acc: 0.9784\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0756 - acc: 0.9787\n",
      "1253/1253 [==============================] - 0s 93us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 93us/step - loss: 0.2155 - acc: 0.9478\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0916 - acc: 0.9731\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0885 - acc: 0.9742\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0897 - acc: 0.9748\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0884 - acc: 0.9740\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0862 - acc: 0.9750\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0874 - acc: 0.9752\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0869 - acc: 0.9747\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0873 - acc: 0.9748\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0867 - acc: 0.9748\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0850 - acc: 0.9757\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0856 - acc: 0.9750\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0848 - acc: 0.9752\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0847 - acc: 0.9749\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0846 - acc: 0.9750\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0832 - acc: 0.9756\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0832 - acc: 0.9759\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0831 - acc: 0.9759\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0828 - acc: 0.9753\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0827 - acc: 0.9748\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0825 - acc: 0.9761\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0823 - acc: 0.9755\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0809 - acc: 0.9763\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0832 - acc: 0.9756\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0820 - acc: 0.9757\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0814 - acc: 0.9764\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0808 - acc: 0.9758\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0810 - acc: 0.9758\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0815 - acc: 0.9769\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0808 - acc: 0.9759\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0811 - acc: 0.9763\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0803 - acc: 0.9758\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0806 - acc: 0.9763\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0802 - acc: 0.9761\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0797 - acc: 0.9763\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0799 - acc: 0.9769\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0795 - acc: 0.9761\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0800 - acc: 0.9765\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0791 - acc: 0.9764\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0802 - acc: 0.9755\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0794 - acc: 0.9774\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0790 - acc: 0.9766\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0791 - acc: 0.9773\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0790 - acc: 0.9774\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0790 - acc: 0.9769\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0783 - acc: 0.9776\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0791 - acc: 0.9765\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0780 - acc: 0.9769\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0779 - acc: 0.9772\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0783 - acc: 0.9766\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0771 - acc: 0.9766\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0779 - acc: 0.9773\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0773 - acc: 0.9773\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0769 - acc: 0.9769\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0765 - acc: 0.9779\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0771 - acc: 0.9771\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0773 - acc: 0.9784\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0770 - acc: 0.9777\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0773 - acc: 0.9776\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0780 - acc: 0.9770\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0774 - acc: 0.9771\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0761 - acc: 0.9776\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0766 - acc: 0.9775\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0754 - acc: 0.9778\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0766 - acc: 0.9772\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0757 - acc: 0.9775\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0753 - acc: 0.9773\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 80us/step - loss: 0.0755 - acc: 0.9775\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 89us/step - loss: 0.0757 - acc: 0.9781\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0758 - acc: 0.9778\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0751 - acc: 0.9781\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0750 - acc: 0.9777\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0740 - acc: 0.9777\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 75us/step - loss: 0.0753 - acc: 0.9774\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 81us/step - loss: 0.0750 - acc: 0.9775\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 78us/step - loss: 0.0743 - acc: 0.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0739 - acc: 0.9781\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0761 - acc: 0.9784\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0754 - acc: 0.9783\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0739 - acc: 0.9780\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0746 - acc: 0.9776\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0745 - acc: 0.9781\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0737 - acc: 0.9777\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0737 - acc: 0.9787\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0747 - acc: 0.9784\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0742 - acc: 0.9785\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0736 - acc: 0.9784\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0733 - acc: 0.9786\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0725 - acc: 0.9790\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0733 - acc: 0.9786\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0735 - acc: 0.9784\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0735 - acc: 0.9784\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0734 - acc: 0.9784\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0726 - acc: 0.9792\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0721 - acc: 0.9786\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0736 - acc: 0.9782\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0731 - acc: 0.9776\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0730 - acc: 0.9784\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0732 - acc: 0.9792\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0725 - acc: 0.9783\n",
      "1253/1253 [==============================] - 0s 110us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 105us/step - loss: 0.2285 - acc: 0.9431\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0949 - acc: 0.9719\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0922 - acc: 0.9734\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0911 - acc: 0.9742\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0906 - acc: 0.9733\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0898 - acc: 0.9732\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0886 - acc: 0.9738\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0885 - acc: 0.9740\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0891 - acc: 0.9741\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0888 - acc: 0.9733\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0880 - acc: 0.9745\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0890 - acc: 0.9738\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0874 - acc: 0.9740\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0874 - acc: 0.9749\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0865 - acc: 0.9746\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0869 - acc: 0.9744\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0860 - acc: 0.9752\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0861 - acc: 0.9748\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0856 - acc: 0.9747\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0859 - acc: 0.9746\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0849 - acc: 0.9757\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0853 - acc: 0.9741\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0838 - acc: 0.9753\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0836 - acc: 0.9751: 0s - loss: 0.0844 - acc: 0.974\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0831 - acc: 0.9755\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0829 - acc: 0.9753\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0834 - acc: 0.9756\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0829 - acc: 0.9751\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0834 - acc: 0.9752\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0831 - acc: 0.9759\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0826 - acc: 0.9753\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0829 - acc: 0.9761\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0823 - acc: 0.9762\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0825 - acc: 0.9758\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0822 - acc: 0.9759\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0815 - acc: 0.9759\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0819 - acc: 0.9761\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0815 - acc: 0.9761\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0817 - acc: 0.9755\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0820 - acc: 0.9758\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0820 - acc: 0.9759\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0812 - acc: 0.9763\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0816 - acc: 0.9758\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0806 - acc: 0.9763\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0819 - acc: 0.9758\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0814 - acc: 0.9761\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0806 - acc: 0.9760\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0805 - acc: 0.9765\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0812 - acc: 0.9767\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0807 - acc: 0.9758\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0810 - acc: 0.9761\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0807 - acc: 0.9758\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0806 - acc: 0.9759\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0807 - acc: 0.9760\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0801 - acc: 0.9764\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0797 - acc: 0.9765\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0806 - acc: 0.9758\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0801 - acc: 0.9764\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0802 - acc: 0.9769\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0798 - acc: 0.9761\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0791 - acc: 0.9762\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0794 - acc: 0.9768\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0790 - acc: 0.9759\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0796 - acc: 0.9763\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0783 - acc: 0.9770\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0788 - acc: 0.9765\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0785 - acc: 0.9769\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0779 - acc: 0.9775\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0779 - acc: 0.9762\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0786 - acc: 0.9773\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0775 - acc: 0.9772\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0784 - acc: 0.9763\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0779 - acc: 0.9768\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0768 - acc: 0.9769\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0784 - acc: 0.9769\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0777 - acc: 0.9763\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0772 - acc: 0.9775\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0776 - acc: 0.9769\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0773 - acc: 0.9769\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0765 - acc: 0.9769\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0770 - acc: 0.9767\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0757 - acc: 0.9776\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0765 - acc: 0.9772\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0771 - acc: 0.9776\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0754 - acc: 0.9773\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0761 - acc: 0.9778\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0755 - acc: 0.9772\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0766 - acc: 0.9774\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0755 - acc: 0.9771\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0755 - acc: 0.9778\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0757 - acc: 0.9776\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0755 - acc: 0.9779\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0749 - acc: 0.9782\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0743 - acc: 0.9783\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0760 - acc: 0.9781\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0753 - acc: 0.9777\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0755 - acc: 0.9773\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0750 - acc: 0.9774\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0744 - acc: 0.9776\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0753 - acc: 0.9779\n",
      "1253/1253 [==============================] - 0s 124us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 2s 133us/step - loss: 0.2329 - acc: 0.9057\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.1351 - acc: 0.9614\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.1235 - acc: 0.9729\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.1187 - acc: 0.9745\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.1148 - acc: 0.9747\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.1098 - acc: 0.9758\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.1062 - acc: 0.9757\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.1038 - acc: 0.9748\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.1010 - acc: 0.9756\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0990 - acc: 0.9751\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0968 - acc: 0.9753\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0953 - acc: 0.9755\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0939 - acc: 0.9759\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0940 - acc: 0.9753\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0921 - acc: 0.9758\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0910 - acc: 0.9755\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0900 - acc: 0.9756\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0891 - acc: 0.9751\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0898 - acc: 0.9756\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0892 - acc: 0.9759\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0878 - acc: 0.9760\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0865 - acc: 0.9761\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0866 - acc: 0.9751\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0862 - acc: 0.9762\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0854 - acc: 0.9754\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0856 - acc: 0.9768\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0860 - acc: 0.9753\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0844 - acc: 0.9755\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0853 - acc: 0.9756\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0843 - acc: 0.9758\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0835 - acc: 0.9754\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0851 - acc: 0.9763\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0835 - acc: 0.9762\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0844 - acc: 0.9763\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0829 - acc: 0.9763\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0837 - acc: 0.9765\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0840 - acc: 0.9754\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0832 - acc: 0.9765\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0821 - acc: 0.9764\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0828 - acc: 0.9760\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0819 - acc: 0.9761\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0824 - acc: 0.9765\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0821 - acc: 0.9765\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0815 - acc: 0.9774\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0819 - acc: 0.9766\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0813 - acc: 0.9763\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0812 - acc: 0.9778\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0815 - acc: 0.9769\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0812 - acc: 0.9763\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0809 - acc: 0.9768\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0804 - acc: 0.9761\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0808 - acc: 0.9770\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 77us/step - loss: 0.0798 - acc: 0.9766\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0802 - acc: 0.9763\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0788 - acc: 0.9767\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0797 - acc: 0.9778\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0790 - acc: 0.9776\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0790 - acc: 0.9764\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0793 - acc: 0.9764\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0799 - acc: 0.9768\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0793 - acc: 0.9777\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0792 - acc: 0.9772\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0794 - acc: 0.9771\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0798 - acc: 0.9781\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0786 - acc: 0.9769\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 53us/step - loss: 0.0784 - acc: 0.9769\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 52us/step - loss: 0.0783 - acc: 0.9776: 0s - loss: 0.0798 - ac\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0779 - acc: 0.9776\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0771 - acc: 0.9774\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0771 - acc: 0.9782\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0792 - acc: 0.9777\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0778 - acc: 0.9775\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0775 - acc: 0.9775\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0775 - acc: 0.9776\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 74us/step - loss: 0.0790 - acc: 0.9769\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0772 - acc: 0.9776\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0767 - acc: 0.9777\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0768 - acc: 0.9780\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0762 - acc: 0.9779\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0764 - acc: 0.9782\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0761 - acc: 0.9774\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0760 - acc: 0.9787\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0762 - acc: 0.9778\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0767 - acc: 0.9785\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0762 - acc: 0.9779\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0764 - acc: 0.9783\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0754 - acc: 0.9778\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0754 - acc: 0.9779\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0751 - acc: 0.9782\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 81us/step - loss: 0.0766 - acc: 0.9781\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 75us/step - loss: 0.0747 - acc: 0.9784\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0761 - acc: 0.9779\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0739 - acc: 0.9796\n",
      "Epoch 94/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0751 - acc: 0.9776\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0740 - acc: 0.9782\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0736 - acc: 0.9788\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0740 - acc: 0.9787\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0747 - acc: 0.9784\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0756 - acc: 0.9782\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0738 - acc: 0.9796\n",
      "1253/1253 [==============================] - 0s 133us/step\n",
      "Epoch 1/100\n",
      "11275/11275 [==============================] - 1s 104us/step - loss: 0.2488 - acc: 0.9039\n",
      "Epoch 2/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.1349 - acc: 0.9464\n",
      "Epoch 3/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.1256 - acc: 0.9718\n",
      "Epoch 4/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.1189 - acc: 0.9737\n",
      "Epoch 5/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.1152 - acc: 0.9745\n",
      "Epoch 6/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.1100 - acc: 0.9748\n",
      "Epoch 7/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.1067 - acc: 0.9762\n",
      "Epoch 8/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.1043 - acc: 0.9746\n",
      "Epoch 9/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.1013 - acc: 0.9750\n",
      "Epoch 10/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0992 - acc: 0.9754\n",
      "Epoch 11/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0971 - acc: 0.9755\n",
      "Epoch 12/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0959 - acc: 0.9753\n",
      "Epoch 13/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0941 - acc: 0.9753\n",
      "Epoch 14/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0929 - acc: 0.9752\n",
      "Epoch 15/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0924 - acc: 0.9747\n",
      "Epoch 16/100\n",
      "11275/11275 [==============================] - 1s 80us/step - loss: 0.0902 - acc: 0.9757\n",
      "Epoch 17/100\n",
      "11275/11275 [==============================] - 1s 75us/step - loss: 0.0895 - acc: 0.9749\n",
      "Epoch 18/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0889 - acc: 0.9756\n",
      "Epoch 19/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0880 - acc: 0.9753\n",
      "Epoch 20/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0885 - acc: 0.9757\n",
      "Epoch 21/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0873 - acc: 0.9755\n",
      "Epoch 22/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0871 - acc: 0.9759\n",
      "Epoch 23/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0862 - acc: 0.9757\n",
      "Epoch 24/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0860 - acc: 0.9760\n",
      "Epoch 25/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0853 - acc: 0.9757\n",
      "Epoch 26/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0864 - acc: 0.9754\n",
      "Epoch 27/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0852 - acc: 0.9754\n",
      "Epoch 28/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0853 - acc: 0.9759\n",
      "Epoch 29/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0847 - acc: 0.9759\n",
      "Epoch 30/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0852 - acc: 0.9759\n",
      "Epoch 31/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0847 - acc: 0.9759\n",
      "Epoch 32/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0847 - acc: 0.9760\n",
      "Epoch 33/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0842 - acc: 0.9755\n",
      "Epoch 34/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0846 - acc: 0.9759\n",
      "Epoch 35/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0847 - acc: 0.9759\n",
      "Epoch 36/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0834 - acc: 0.9763\n",
      "Epoch 37/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0841 - acc: 0.9764\n",
      "Epoch 38/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0836 - acc: 0.9766\n",
      "Epoch 39/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0835 - acc: 0.9755\n",
      "Epoch 40/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0829 - acc: 0.9756\n",
      "Epoch 41/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0832 - acc: 0.9761\n",
      "Epoch 42/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0834 - acc: 0.9764\n",
      "Epoch 43/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0842 - acc: 0.9758\n",
      "Epoch 44/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0844 - acc: 0.9762\n",
      "Epoch 45/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0832 - acc: 0.9758\n",
      "Epoch 46/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0829 - acc: 0.9762\n",
      "Epoch 47/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0841 - acc: 0.9761\n",
      "Epoch 48/100\n",
      "11275/11275 [==============================] - 1s 73us/step - loss: 0.0829 - acc: 0.9763\n",
      "Epoch 49/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0826 - acc: 0.9761\n",
      "Epoch 50/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0818 - acc: 0.9759\n",
      "Epoch 51/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0827 - acc: 0.9762\n",
      "Epoch 52/100\n",
      "11275/11275 [==============================] - 1s 81us/step - loss: 0.0825 - acc: 0.9761\n",
      "Epoch 53/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0824 - acc: 0.9761\n",
      "Epoch 54/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0821 - acc: 0.9764\n",
      "Epoch 55/100\n",
      "11275/11275 [==============================] - 1s 61us/step - loss: 0.0813 - acc: 0.9763\n",
      "Epoch 56/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0818 - acc: 0.9772\n",
      "Epoch 57/100\n",
      "11275/11275 [==============================] - 1s 71us/step - loss: 0.0818 - acc: 0.9763\n",
      "Epoch 58/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0819 - acc: 0.9776\n",
      "Epoch 59/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0816 - acc: 0.9765\n",
      "Epoch 60/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0811 - acc: 0.9761\n",
      "Epoch 61/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0802 - acc: 0.9768\n",
      "Epoch 62/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0803 - acc: 0.9766\n",
      "Epoch 63/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0806 - acc: 0.9765\n",
      "Epoch 64/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0806 - acc: 0.9771\n",
      "Epoch 65/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0798 - acc: 0.9766\n",
      "Epoch 66/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0801 - acc: 0.9761\n",
      "Epoch 67/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0790 - acc: 0.9769\n",
      "Epoch 68/100\n",
      "11275/11275 [==============================] - 1s 67us/step - loss: 0.0797 - acc: 0.9773\n",
      "Epoch 69/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0795 - acc: 0.9768\n",
      "Epoch 70/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0796 - acc: 0.9763\n",
      "Epoch 71/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0797 - acc: 0.9765\n",
      "Epoch 72/100\n",
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0795 - acc: 0.9769\n",
      "Epoch 73/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0788 - acc: 0.9769\n",
      "Epoch 74/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0801 - acc: 0.9766\n",
      "Epoch 75/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0790 - acc: 0.9777\n",
      "Epoch 76/100\n",
      "11275/11275 [==============================] - 1s 64us/step - loss: 0.0782 - acc: 0.9770\n",
      "Epoch 77/100\n",
      "11275/11275 [==============================] - 1s 65us/step - loss: 0.0788 - acc: 0.9783\n",
      "Epoch 78/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0784 - acc: 0.9780\n",
      "Epoch 79/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0782 - acc: 0.9771\n",
      "Epoch 80/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0779 - acc: 0.9773\n",
      "Epoch 81/100\n",
      "11275/11275 [==============================] - 1s 72us/step - loss: 0.0771 - acc: 0.9773\n",
      "Epoch 82/100\n",
      "11275/11275 [==============================] - 1s 88us/step - loss: 0.0778 - acc: 0.9770\n",
      "Epoch 83/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0780 - acc: 0.9768\n",
      "Epoch 84/100\n",
      "11275/11275 [==============================] - 1s 69us/step - loss: 0.0769 - acc: 0.9775\n",
      "Epoch 85/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0781 - acc: 0.9773\n",
      "Epoch 86/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0770 - acc: 0.9773\n",
      "Epoch 87/100\n",
      "11275/11275 [==============================] - 1s 55us/step - loss: 0.0766 - acc: 0.9776\n",
      "Epoch 88/100\n",
      "11275/11275 [==============================] - 1s 58us/step - loss: 0.0770 - acc: 0.9776\n",
      "Epoch 89/100\n",
      "11275/11275 [==============================] - 1s 57us/step - loss: 0.0765 - acc: 0.9776\n",
      "Epoch 90/100\n",
      "11275/11275 [==============================] - 1s 56us/step - loss: 0.0767 - acc: 0.9774\n",
      "Epoch 91/100\n",
      "11275/11275 [==============================] - 1s 54us/step - loss: 0.0767 - acc: 0.9776\n",
      "Epoch 92/100\n",
      "11275/11275 [==============================] - 1s 60us/step - loss: 0.0756 - acc: 0.9787\n",
      "Epoch 93/100\n",
      "11275/11275 [==============================] - 1s 63us/step - loss: 0.0769 - acc: 0.9781\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11275/11275 [==============================] - 1s 66us/step - loss: 0.0755 - acc: 0.9773\n",
      "Epoch 95/100\n",
      "11275/11275 [==============================] - 1s 70us/step - loss: 0.0754 - acc: 0.9777\n",
      "Epoch 96/100\n",
      "11275/11275 [==============================] - 1s 62us/step - loss: 0.0763 - acc: 0.9776\n",
      "Epoch 97/100\n",
      "11275/11275 [==============================] - 1s 68us/step - loss: 0.0755 - acc: 0.9780\n",
      "Epoch 98/100\n",
      "11275/11275 [==============================] - 1s 59us/step - loss: 0.0749 - acc: 0.9779\n",
      "Epoch 99/100\n",
      "11275/11275 [==============================] - 1s 77us/step - loss: 0.0763 - acc: 0.9784\n",
      "Epoch 100/100\n",
      "11275/11275 [==============================] - 1s 85us/step - loss: 0.0751 - acc: 0.9779\n",
      "1253/1253 [==============================] - 0s 145us/step\n",
      "Epoch 1/100\n",
      "11276/11276 [==============================] - 1s 106us/step - loss: 0.2350 - acc: 0.9048\n",
      "Epoch 2/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.1371 - acc: 0.9580\n",
      "Epoch 3/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.1261 - acc: 0.9714\n",
      "Epoch 4/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.1206 - acc: 0.9735\n",
      "Epoch 5/100\n",
      "11276/11276 [==============================] - 1s 72us/step - loss: 0.1157 - acc: 0.9744\n",
      "Epoch 6/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.1110 - acc: 0.9754\n",
      "Epoch 7/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.1075 - acc: 0.9745\n",
      "Epoch 8/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.1053 - acc: 0.9745\n",
      "Epoch 9/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.1026 - acc: 0.9747\n",
      "Epoch 10/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.1004 - acc: 0.9755\n",
      "Epoch 11/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0979 - acc: 0.9749\n",
      "Epoch 12/100\n",
      "11276/11276 [==============================] - 1s 72us/step - loss: 0.0969 - acc: 0.9753\n",
      "Epoch 13/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0956 - acc: 0.9748\n",
      "Epoch 14/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0939 - acc: 0.9749\n",
      "Epoch 15/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0927 - acc: 0.9745\n",
      "Epoch 16/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0919 - acc: 0.9755\n",
      "Epoch 17/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0909 - acc: 0.9752\n",
      "Epoch 18/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0907 - acc: 0.9754\n",
      "Epoch 19/100\n",
      "11276/11276 [==============================] - 1s 81us/step - loss: 0.0906 - acc: 0.9752\n",
      "Epoch 20/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0892 - acc: 0.9751\n",
      "Epoch 21/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0888 - acc: 0.9753\n",
      "Epoch 22/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0886 - acc: 0.9758\n",
      "Epoch 23/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0886 - acc: 0.9752\n",
      "Epoch 24/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0873 - acc: 0.9754\n",
      "Epoch 25/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0877 - acc: 0.9751\n",
      "Epoch 26/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0869 - acc: 0.9754\n",
      "Epoch 27/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0873 - acc: 0.9757\n",
      "Epoch 28/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0871 - acc: 0.9756\n",
      "Epoch 29/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0860 - acc: 0.9757\n",
      "Epoch 30/100\n",
      "11276/11276 [==============================] - 1s 70us/step - loss: 0.0873 - acc: 0.9752\n",
      "Epoch 31/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0855 - acc: 0.9758\n",
      "Epoch 32/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0856 - acc: 0.9753\n",
      "Epoch 33/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0856 - acc: 0.9752\n",
      "Epoch 34/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0851 - acc: 0.9748\n",
      "Epoch 35/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0854 - acc: 0.9753\n",
      "Epoch 36/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0846 - acc: 0.9761\n",
      "Epoch 37/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0842 - acc: 0.9761\n",
      "Epoch 38/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0844 - acc: 0.9759\n",
      "Epoch 39/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0854 - acc: 0.9756\n",
      "Epoch 40/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0838 - acc: 0.9761\n",
      "Epoch 41/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0842 - acc: 0.9770\n",
      "Epoch 42/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0843 - acc: 0.9756\n",
      "Epoch 43/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0842 - acc: 0.9753\n",
      "Epoch 44/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0827 - acc: 0.9757\n",
      "Epoch 45/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0834 - acc: 0.9761\n",
      "Epoch 46/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0841 - acc: 0.9754\n",
      "Epoch 47/100\n",
      "11276/11276 [==============================] - 1s 70us/step - loss: 0.0827 - acc: 0.9765\n",
      "Epoch 48/100\n",
      "11276/11276 [==============================] - 1s 76us/step - loss: 0.0831 - acc: 0.9763\n",
      "Epoch 49/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0825 - acc: 0.9764\n",
      "Epoch 50/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0823 - acc: 0.9773\n",
      "Epoch 51/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0828 - acc: 0.9760\n",
      "Epoch 52/100\n",
      "11276/11276 [==============================] - 1s 56us/step - loss: 0.0836 - acc: 0.9767\n",
      "Epoch 53/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0825 - acc: 0.9766\n",
      "Epoch 54/100\n",
      "11276/11276 [==============================] - 1s 84us/step - loss: 0.0816 - acc: 0.9771\n",
      "Epoch 55/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0817 - acc: 0.9764\n",
      "Epoch 56/100\n",
      "11276/11276 [==============================] - 1s 57us/step - loss: 0.0827 - acc: 0.9764\n",
      "Epoch 57/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0814 - acc: 0.9760\n",
      "Epoch 58/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0822 - acc: 0.9764\n",
      "Epoch 59/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0813 - acc: 0.9770\n",
      "Epoch 60/100\n",
      "11276/11276 [==============================] - 1s 56us/step - loss: 0.0823 - acc: 0.9768\n",
      "Epoch 61/100\n",
      "11276/11276 [==============================] - 1s 56us/step - loss: 0.0812 - acc: 0.9777\n",
      "Epoch 62/100\n",
      "11276/11276 [==============================] - 1s 56us/step - loss: 0.0816 - acc: 0.9768\n",
      "Epoch 63/100\n",
      "11276/11276 [==============================] - 1s 56us/step - loss: 0.0801 - acc: 0.9774\n",
      "Epoch 64/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0811 - acc: 0.9777\n",
      "Epoch 65/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0807 - acc: 0.9770\n",
      "Epoch 66/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0809 - acc: 0.9771\n",
      "Epoch 67/100\n",
      "11276/11276 [==============================] - 1s 75us/step - loss: 0.0802 - acc: 0.9773\n",
      "Epoch 68/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0794 - acc: 0.9766\n",
      "Epoch 69/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0802 - acc: 0.9775\n",
      "Epoch 70/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0810 - acc: 0.9771\n",
      "Epoch 71/100\n",
      "11276/11276 [==============================] - 1s 73us/step - loss: 0.0787 - acc: 0.9780\n",
      "Epoch 72/100\n",
      "11276/11276 [==============================] - 1s 71us/step - loss: 0.0791 - acc: 0.9773\n",
      "Epoch 73/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0792 - acc: 0.9776\n",
      "Epoch 74/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0788 - acc: 0.9768\n",
      "Epoch 75/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0790 - acc: 0.9780\n",
      "Epoch 76/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0791 - acc: 0.9773\n",
      "Epoch 77/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0783 - acc: 0.9770\n",
      "Epoch 78/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0794 - acc: 0.9771\n",
      "Epoch 79/100\n",
      "11276/11276 [==============================] - 1s 95us/step - loss: 0.0785 - acc: 0.9772\n",
      "Epoch 80/100\n",
      "11276/11276 [==============================] - 1s 70us/step - loss: 0.0785 - acc: 0.9773\n",
      "Epoch 81/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0775 - acc: 0.9784\n",
      "Epoch 82/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0774 - acc: 0.9783\n",
      "Epoch 83/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0775 - acc: 0.9779\n",
      "Epoch 84/100\n",
      "11276/11276 [==============================] - 1s 81us/step - loss: 0.0775 - acc: 0.9779\n",
      "Epoch 85/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0786 - acc: 0.9771\n",
      "Epoch 86/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0772 - acc: 0.9779\n",
      "Epoch 87/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0771 - acc: 0.9770\n",
      "Epoch 88/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0771 - acc: 0.9775\n",
      "Epoch 89/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0761 - acc: 0.9784\n",
      "Epoch 90/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0778 - acc: 0.9776\n",
      "Epoch 91/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0769 - acc: 0.9777\n",
      "Epoch 92/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0761 - acc: 0.9776\n",
      "Epoch 93/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0767 - acc: 0.9786\n",
      "Epoch 94/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0764 - acc: 0.9786\n",
      "Epoch 95/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0768 - acc: 0.9783\n",
      "Epoch 96/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0764 - acc: 0.9788\n",
      "Epoch 97/100\n",
      "11276/11276 [==============================] - 1s 89us/step - loss: 0.0772 - acc: 0.9777\n",
      "Epoch 98/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0756 - acc: 0.9786\n",
      "Epoch 99/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0768 - acc: 0.9777\n",
      "Epoch 100/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0753 - acc: 0.9788\n",
      "1252/1252 [==============================] - 0s 162us/step\n",
      "Epoch 1/100\n",
      "11276/11276 [==============================] - 1s 105us/step - loss: 0.2822 - acc: 0.9275\n",
      "Epoch 2/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0992 - acc: 0.9703\n",
      "Epoch 3/100\n",
      "11276/11276 [==============================] - 1s 98us/step - loss: 0.0921 - acc: 0.9732\n",
      "Epoch 4/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0909 - acc: 0.9737\n",
      "Epoch 5/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0905 - acc: 0.9737\n",
      "Epoch 6/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0916 - acc: 0.9740\n",
      "Epoch 7/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0901 - acc: 0.9737\n",
      "Epoch 8/100\n",
      "11276/11276 [==============================] - 1s 57us/step - loss: 0.0893 - acc: 0.9739\n",
      "Epoch 9/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0892 - acc: 0.9745\n",
      "Epoch 10/100\n",
      "11276/11276 [==============================] - 1s 83us/step - loss: 0.0891 - acc: 0.9743\n",
      "Epoch 11/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0883 - acc: 0.9745\n",
      "Epoch 12/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0883 - acc: 0.9737\n",
      "Epoch 13/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0883 - acc: 0.9746\n",
      "Epoch 14/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0879 - acc: 0.9745\n",
      "Epoch 15/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0877 - acc: 0.9746\n",
      "Epoch 16/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0872 - acc: 0.9745\n",
      "Epoch 17/100\n",
      "11276/11276 [==============================] - 1s 76us/step - loss: 0.0872 - acc: 0.9750\n",
      "Epoch 18/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0867 - acc: 0.9743\n",
      "Epoch 19/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0864 - acc: 0.9747\n",
      "Epoch 20/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0857 - acc: 0.9738\n",
      "Epoch 21/100\n",
      "11276/11276 [==============================] - 1s 80us/step - loss: 0.0856 - acc: 0.9746\n",
      "Epoch 22/100\n",
      "11276/11276 [==============================] - 1s 84us/step - loss: 0.0851 - acc: 0.9753\n",
      "Epoch 23/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0845 - acc: 0.9750\n",
      "Epoch 24/100\n",
      "11276/11276 [==============================] - 1s 86us/step - loss: 0.0851 - acc: 0.9745\n",
      "Epoch 25/100\n",
      "11276/11276 [==============================] - 1s 82us/step - loss: 0.0841 - acc: 0.9750\n",
      "Epoch 26/100\n",
      "11276/11276 [==============================] - 1s 75us/step - loss: 0.0839 - acc: 0.9754\n",
      "Epoch 27/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.0842 - acc: 0.9755\n",
      "Epoch 28/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0834 - acc: 0.9749\n",
      "Epoch 29/100\n",
      "11276/11276 [==============================] - 1s 57us/step - loss: 0.0833 - acc: 0.9754\n",
      "Epoch 30/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0830 - acc: 0.9754\n",
      "Epoch 31/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0835 - acc: 0.9752\n",
      "Epoch 32/100\n",
      "11276/11276 [==============================] - 1s 57us/step - loss: 0.0828 - acc: 0.9760\n",
      "Epoch 33/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.0824 - acc: 0.9747\n",
      "Epoch 34/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.0830 - acc: 0.9748\n",
      "Epoch 35/100\n",
      "11276/11276 [==============================] - 1s 74us/step - loss: 0.0832 - acc: 0.9753\n",
      "Epoch 36/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0820 - acc: 0.9756\n",
      "Epoch 37/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0826 - acc: 0.9755\n",
      "Epoch 38/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0822 - acc: 0.9756\n",
      "Epoch 39/100\n",
      "11276/11276 [==============================] - 1s 59us/step - loss: 0.0818 - acc: 0.9750\n",
      "Epoch 40/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0815 - acc: 0.9758\n",
      "Epoch 41/100\n",
      "11276/11276 [==============================] - 1s 54us/step - loss: 0.0817 - acc: 0.9753\n",
      "Epoch 42/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0816 - acc: 0.9756\n",
      "Epoch 43/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0810 - acc: 0.9764\n",
      "Epoch 44/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0810 - acc: 0.9760\n",
      "Epoch 45/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0824 - acc: 0.9759\n",
      "Epoch 46/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0813 - acc: 0.9761\n",
      "Epoch 47/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0807 - acc: 0.9771\n",
      "Epoch 48/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0810 - acc: 0.9767\n",
      "Epoch 49/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0810 - acc: 0.9759\n",
      "Epoch 50/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0802 - acc: 0.9761\n",
      "Epoch 51/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0811 - acc: 0.9758\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0800 - acc: 0.9757\n",
      "Epoch 53/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0794 - acc: 0.9762\n",
      "Epoch 54/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0799 - acc: 0.9770\n",
      "Epoch 55/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0798 - acc: 0.9762\n",
      "Epoch 56/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0791 - acc: 0.9768\n",
      "Epoch 57/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0793 - acc: 0.9768\n",
      "Epoch 58/100\n",
      "11276/11276 [==============================] - 1s 71us/step - loss: 0.0790 - acc: 0.9769\n",
      "Epoch 59/100\n",
      "11276/11276 [==============================] - 1s 78us/step - loss: 0.0796 - acc: 0.9764\n",
      "Epoch 60/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0792 - acc: 0.9761\n",
      "Epoch 61/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0790 - acc: 0.9767\n",
      "Epoch 62/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0791 - acc: 0.9761\n",
      "Epoch 63/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0784 - acc: 0.9771\n",
      "Epoch 64/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0785 - acc: 0.9761\n",
      "Epoch 65/100\n",
      "11276/11276 [==============================] - 1s 60us/step - loss: 0.0791 - acc: 0.9764\n",
      "Epoch 66/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0780 - acc: 0.9770\n",
      "Epoch 67/100\n",
      "11276/11276 [==============================] - 1s 67us/step - loss: 0.0788 - acc: 0.9769\n",
      "Epoch 68/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0784 - acc: 0.9772\n",
      "Epoch 69/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0780 - acc: 0.9776\n",
      "Epoch 70/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0790 - acc: 0.9769\n",
      "Epoch 71/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0778 - acc: 0.9777\n",
      "Epoch 72/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0773 - acc: 0.9772\n",
      "Epoch 73/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0777 - acc: 0.9777\n",
      "Epoch 74/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0777 - acc: 0.9773\n",
      "Epoch 75/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0773 - acc: 0.9781: 0s - loss: 0.0775\n",
      "Epoch 76/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0780 - acc: 0.9778\n",
      "Epoch 77/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0770 - acc: 0.9772\n",
      "Epoch 78/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0776 - acc: 0.9767\n",
      "Epoch 79/100\n",
      "11276/11276 [==============================] - 1s 69us/step - loss: 0.0765 - acc: 0.9777\n",
      "Epoch 80/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0768 - acc: 0.9773\n",
      "Epoch 81/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0769 - acc: 0.9771\n",
      "Epoch 82/100\n",
      "11276/11276 [==============================] - 1s 58us/step - loss: 0.0767 - acc: 0.9776\n",
      "Epoch 83/100\n",
      "11276/11276 [==============================] - 1s 61us/step - loss: 0.0769 - acc: 0.9777\n",
      "Epoch 84/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0765 - acc: 0.9770\n",
      "Epoch 85/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0767 - acc: 0.9781\n",
      "Epoch 86/100\n",
      "11276/11276 [==============================] - 1s 77us/step - loss: 0.0762 - acc: 0.9770\n",
      "Epoch 87/100\n",
      "11276/11276 [==============================] - 1s 71us/step - loss: 0.0767 - acc: 0.9772\n",
      "Epoch 88/100\n",
      "11276/11276 [==============================] - 1s 64us/step - loss: 0.0752 - acc: 0.9786\n",
      "Epoch 89/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0756 - acc: 0.9777\n",
      "Epoch 90/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0759 - acc: 0.9770\n",
      "Epoch 91/100\n",
      "11276/11276 [==============================] - 1s 72us/step - loss: 0.0758 - acc: 0.9777\n",
      "Epoch 92/100\n",
      "11276/11276 [==============================] - 1s 70us/step - loss: 0.0769 - acc: 0.9766\n",
      "Epoch 93/100\n",
      "11276/11276 [==============================] - 1s 66us/step - loss: 0.0754 - acc: 0.9768\n",
      "Epoch 94/100\n",
      "11276/11276 [==============================] - 1s 65us/step - loss: 0.0757 - acc: 0.9774\n",
      "Epoch 95/100\n",
      "11276/11276 [==============================] - 1s 71us/step - loss: 0.0759 - acc: 0.9773\n",
      "Epoch 96/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0757 - acc: 0.9773\n",
      "Epoch 97/100\n",
      "11276/11276 [==============================] - 1s 68us/step - loss: 0.0752 - acc: 0.9783\n",
      "Epoch 98/100\n",
      "11276/11276 [==============================] - 1s 63us/step - loss: 0.0758 - acc: 0.9775\n",
      "Epoch 99/100\n",
      "11276/11276 [==============================] - 1s 80us/step - loss: 0.0761 - acc: 0.9771\n",
      "Epoch 100/100\n",
      "11276/11276 [==============================] - 1s 62us/step - loss: 0.0748 - acc: 0.9778\n",
      "1252/1252 [==============================] - 0s 180us/step\n",
      "Accuracy mean: 0.977171274564\n",
      "Accuracy variance: 0.00519865787179\n"
     ]
    }
   ],
   "source": [
    "# estimators \n",
    "# Evaluating the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DecisionTreeClassifier = 96.778399 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of DecisionTreeClassifier = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of RandomForestClassifier = 98.044693 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of RandomForestClassifier = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of ExtraTreesClassifier = 97.895717 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of ExtraTreesClassifier = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of AdaBoostClassifier = 97.802607 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of AdaBoostClassifier = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive_bayes  GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GaussianNB = 94.897579 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred= gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of GaussianNB = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
